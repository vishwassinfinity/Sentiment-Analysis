{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6866e64b",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis with NLP\n",
    "## A Complete Machine Learning Pipeline for Social Media Text Classification\n",
    "\n",
    "Welcome to this comprehensive tutorial on Twitter sentiment analysis! In this notebook, we'll build a complete Natural Language Processing (NLP) pipeline to analyze and classify sentiments from Twitter data.\n",
    "\n",
    "### 🎯 Objectives\n",
    "- Build an end-to-end sentiment analysis system\n",
    "- Explore and preprocess Twitter data\n",
    "- Extract meaningful features from text\n",
    "- Train and compare multiple machine learning models\n",
    "- Evaluate model performance with comprehensive metrics\n",
    "- Create a production-ready prediction system\n",
    "\n",
    "### 🛠 Tools & Technologies\n",
    "- **Python**: Core programming language\n",
    "- **Pandas**: Data manipulation and analysis\n",
    "- **Scikit-Learn**: Machine learning algorithms and evaluation\n",
    "- **NLTK**: Natural language processing toolkit\n",
    "- **Matplotlib & Seaborn**: Data visualization\n",
    "- **NumPy**: Numerical computations\n",
    "\n",
    "### 📊 Dataset\n",
    "We'll work with a Twitter dataset containing tweets labeled with three sentiment categories:\n",
    "- **Positive**: Happy, satisfied, enthusiastic tweets\n",
    "- **Negative**: Angry, disappointed, critical tweets  \n",
    "- **Neutral**: Factual, informational, or neutral tweets\n",
    "\n",
    "Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5940de8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our sentiment analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17483305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                            confusion_matrix, f1_score, precision_score, \n",
    "                            recall_score)\n",
    "\n",
    "# Text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Additional utilities\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📦 Pandas version: {pd.__version__}\")\n",
    "print(f\"📦 NumPy version: {np.__version__}\")\n",
    "print(f\"📦 Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c29a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    print(\"✅ NLTK data already available\")\n",
    "except LookupError:\n",
    "    print(\"📥 Downloading NLTK data...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    print(\"✅ NLTK data downloaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d718d5",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Twitter Dataset\n",
    "\n",
    "Let's load our Twitter dataset and perform exploratory data analysis to understand the structure and distribution of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Twitter dataset for demonstration\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample Twitter dataset for sentiment analysis\"\"\"\n",
    "    \n",
    "    # Sample tweets for each sentiment\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            # Positive tweets\n",
    "            \"I absolutely love this new movie! Best film of the year! 🎬✨\",\n",
    "            \"Amazing product! Exceeded all my expectations. Highly recommend! 👍\",\n",
    "            \"Such a beautiful day today! Perfect weather for a walk 🌞\",\n",
    "            \"Just got promoted at work! So excited and grateful! 🎉\",\n",
    "            \"This restaurant has the most delicious food ever! 🍕\",\n",
    "            \"Great customer service! The staff was so helpful 😊\",\n",
    "            \"Successfully completed my marathon today! Feeling accomplished! 🏃‍♂️\",\n",
    "            \"Love spending time with family. Best weekend ever! ❤️\",\n",
    "            \"This book is incredible! Can't put it down 📚\",\n",
    "            \"Perfect vacation! Beautiful beaches and sunsets 🏖️\",\n",
    "            \n",
    "            # Negative tweets  \n",
    "            \"Terrible movie! Complete waste of time and money 😡\",\n",
    "            \"Worst customer service ever! Rude staff and long waits 😤\",\n",
    "            \"This product is broken and useless. Total disappointment!\",\n",
    "            \"Horrible weather today! Rain ruined all my plans ☔\",\n",
    "            \"Got stuck in traffic for 2 hours! So frustrating 🚗\",\n",
    "            \"This restaurant has terrible food! Overpriced and tasteless\",\n",
    "            \"Feeling sick and exhausted. Worst day ever! 😷\",\n",
    "            \"Computer crashed and lost all my work! So angry 💻\",\n",
    "            \"Cancelled flight ruined vacation plans. Terrible service ✈️\",\n",
    "            \"This book is boring and poorly written 📖\",\n",
    "            \n",
    "            # Neutral tweets\n",
    "            \"The movie was okay. Not great but not terrible either\",\n",
    "            \"Weather is average today. Neither sunny nor rainy ⛅\",\n",
    "            \"This product works as expected. Nothing special\",\n",
    "            \"Had lunch at a new restaurant. Food was decent\",\n",
    "            \"Regular day at work. Nothing exciting happened\",\n",
    "            \"The book is fine. Some interesting parts 📚\",\n",
    "            \"Traffic was normal today. No major delays\",\n",
    "            \"This coffee shop is alright. Standard service ☕\",\n",
    "            \"Average shopping experience. Found what I needed\",\n",
    "            \"The concert was okay. Not the best but decent 🎵\"\n",
    "        ],\n",
    "        'sentiment': (\n",
    "            ['positive'] * 10 + \n",
    "            ['negative'] * 10 + \n",
    "            ['neutral'] * 10\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(sample_data)\n",
    "\n",
    "# Create and load the dataset\n",
    "df = create_sample_dataset()\n",
    "\n",
    "print(\"📊 Dataset Overview:\")\n",
    "print(f\"Total number of tweets: {len(df)}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n📝 Sample tweets:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore sentiment distribution\n",
    "print(\"📈 Sentiment Distribution:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Calculate percentages\n",
    "sentiment_percentages = df['sentiment'].value_counts(normalize=True) * 100\n",
    "print(\"\\n📊 Sentiment Percentages:\")\n",
    "for sentiment, percentage in sentiment_percentages.items():\n",
    "    print(f\"{sentiment.capitalize()}: {percentage:.1f}%\")\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot\n",
    "sentiment_counts.plot(kind='bar', ax=ax1, color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "ax1.set_title('Sentiment Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Sentiment')\n",
    "ax1.set_ylabel('Number of Tweets')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "        colors=['#ff6b6b', '#4ecdc4', '#45b7d1'], startangle=90)\n",
    "ax2.set_title('Sentiment Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"\\n📏 Text Length Statistics:\")\n",
    "print(f\"Average character length: {df['text_length'].mean():.1f}\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f}\")\n",
    "print(f\"Min length: {df['text_length'].min()}\")\n",
    "print(f\"Max length: {df['text_length'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153540d0",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning\n",
    "\n",
    "Now let's clean our dataset by handling any data quality issues and preparing it for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data quality issues\n",
    "print(\"🔍 Data Quality Check:\")\n",
    "print(f\"Missing values in 'text': {df['text'].isnull().sum()}\")\n",
    "print(f\"Missing values in 'sentiment': {df['sentiment'].isnull().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"Empty text entries: {(df['text'].str.strip() == '').sum()}\")\n",
    "\n",
    "# Check for any unexpected sentiment labels\n",
    "print(f\"\\nUnique sentiment labels: {df['sentiment'].unique()}\")\n",
    "\n",
    "# Remove any rows with missing or empty text\n",
    "initial_count = len(df)\n",
    "df = df.dropna(subset=['text', 'sentiment'])\n",
    "df = df[df['text'].str.strip() != '']\n",
    "final_count = len(df)\n",
    "\n",
    "if initial_count != final_count:\n",
    "    print(f\"⚠️ Removed {initial_count - final_count} rows with missing/empty data\")\n",
    "else:\n",
    "    print(\"✅ No missing or empty data found\")\n",
    "\n",
    "# Ensure sentiment labels are standardized\n",
    "df['sentiment'] = df['sentiment'].str.lower().str.strip()\n",
    "\n",
    "print(f\"\\n📊 Clean dataset shape: {df.shape}\")\n",
    "print(\"✅ Data preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d80722",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing for NLP\n",
    "\n",
    "This is the most crucial step for sentiment analysis! We'll clean and normalize the tweet text to make it suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing function for tweets\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove @mentions and #hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace, newlines, and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def advanced_preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Advanced preprocessing with tokenization and stopword removal\n",
    "    \"\"\"\n",
    "    # Apply basic preprocessing first\n",
    "    text = preprocess_text(text)\n",
    "    \n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Demonstrate preprocessing on sample tweets\n",
    "print(\"🔧 Text Preprocessing Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sample_tweets = [\n",
    "    \"I LOVE this amazing movie!!! 😍😍😍 #amazing #movienight https://example.com @username\",\n",
    "    \"This product is sooooo bad... I want my money back!!! 😡\",\n",
    "    \"Can't believe how good this restaurant is! Best food ever! 🍕👌\"\n",
    "]\n",
    "\n",
    "for i, tweet in enumerate(sample_tweets, 1):\n",
    "    basic_processed = preprocess_text(tweet)\n",
    "    advanced_processed = advanced_preprocess_text(tweet)\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Original:    {tweet}\")\n",
    "    print(f\"Basic:       {basic_processed}\")\n",
    "    print(f\"Advanced:    {advanced_processed}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to our dataset\n",
    "print(\"🔄 Applying text preprocessing to dataset...\")\n",
    "\n",
    "# Apply advanced preprocessing\n",
    "df['text_processed'] = df['text'].apply(advanced_preprocess_text)\n",
    "\n",
    "# Remove any tweets that became empty after preprocessing\n",
    "initial_count = len(df)\n",
    "df = df[df['text_processed'].str.len() > 0]\n",
    "final_count = len(df)\n",
    "\n",
    "if initial_count != final_count:\n",
    "    print(f\"⚠️ Removed {initial_count - final_count} tweets that became empty after preprocessing\")\n",
    "\n",
    "print(f\"✅ Preprocessing completed! {len(df)} tweets ready for analysis\")\n",
    "\n",
    "# Show before and after examples\n",
    "print(\"\\n📝 Before vs After Preprocessing Examples:\")\n",
    "sample_indices = df.head(3).index\n",
    "for idx in sample_indices:\n",
    "    print(f\"\\nOriginal:  {df.loc[idx, 'text']}\")\n",
    "    print(f\"Processed: {df.loc[idx, 'text_processed']}\")\n",
    "    print(f\"Sentiment: {df.loc[idx, 'sentiment']}\")\n",
    "\n",
    "# Check processed text statistics\n",
    "df['processed_length'] = df['text_processed'].str.len()\n",
    "df['processed_word_count'] = df['text_processed'].str.split().str.len()\n",
    "\n",
    "print(f\"\\n📊 Processed Text Statistics:\")\n",
    "print(f\"Average processed length: {df['processed_length'].mean():.1f}\")\n",
    "print(f\"Average processed word count: {df['processed_word_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7d45c",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "Now we'll convert our preprocessed text into numerical features that machine learning algorithms can understand. We'll use TF-IDF (Term Frequency-Inverse Document Frequency) vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "print(\"🔢 Converting text to numerical features using TF-IDF...\")\n",
    "\n",
    "# We'll use TF-IDF with both unigrams and bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # Limit to top 1000 features for our small dataset\n",
    "    ngram_range=(1, 2),  # Use both unigrams and bigrams\n",
    "    min_df=1,  # Minimum document frequency (adjusted for small dataset)\n",
    "    max_df=0.95,  # Maximum document frequency\n",
    "    stop_words='english'  # Remove English stopwords\n",
    ")\n",
    "\n",
    "# Fit and transform the preprocessed text\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['text_processed'])\n",
    "\n",
    "print(f\"✅ TF-IDF Feature Matrix Shape: {X_tfidf.shape}\")\n",
    "print(f\"📊 Number of unique features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"📊 Matrix sparsity: {(1.0 - X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1])):.3f}\")\n",
    "\n",
    "# Get the most important features (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate mean TF-IDF scores for each feature\n",
    "mean_scores = np.array(X_tfidf.mean(axis=0)).flatten()\n",
    "\n",
    "# Get top features\n",
    "top_indices = mean_scores.argsort()[-20:][::-1]\n",
    "top_features = [(feature_names[i], mean_scores[i]) for i in top_indices]\n",
    "\n",
    "print(f\"\\n🔝 Top 10 TF-IDF Features:\")\n",
    "for feature, score in top_features[:10]:\n",
    "    print(f\"  {feature}: {score:.4f}\")\n",
    "\n",
    "# Also try CountVectorizer for comparison\n",
    "print(f\"\\n🔢 Comparing with Count Vectorizer...\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    max_df=0.95,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_count = count_vectorizer.fit_transform(df['text_processed'])\n",
    "\n",
    "print(f\"✅ Count Vectorizer Matrix Shape: {X_count.shape}\")\n",
    "print(f\"📊 Matrix sparsity: {(1.0 - X_count.nnz / (X_count.shape[0] * X_count.shape[1])):.3f}\")\n",
    "\n",
    "# We'll proceed with TF-IDF for our main analysis\n",
    "X = X_tfidf\n",
    "y = df['sentiment']\n",
    "\n",
    "print(f\"\\n🎯 Final feature matrix: {X.shape}\")\n",
    "print(f\"🎯 Target labels: {len(y)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bee8e",
   "metadata": {},
   "source": [
    "## 6. Split Dataset for Training and Testing\n",
    "\n",
    "Let's split our data into training and testing sets to properly evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c788576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "print(\"📊 Splitting data into training and testing sets...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,  # 30% for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y  # Ensure balanced distribution across train/test\n",
    ")\n",
    "\n",
    "print(f\"✅ Data split completed!\")\n",
    "print(f\"📊 Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"📊 Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check sentiment distribution in train and test sets\n",
    "print(f\"\\n🎯 Training set sentiment distribution:\")\n",
    "train_sentiment_dist = pd.Series(y_train).value_counts()\n",
    "for sentiment, count in train_sentiment_dist.items():\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"  {sentiment.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 Testing set sentiment distribution:\")\n",
    "test_sentiment_dist = pd.Series(y_test).value_counts()\n",
    "for sentiment, count in test_sentiment_dist.items():\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"  {sentiment.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize the split\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set distribution\n",
    "train_sentiment_dist.plot(kind='bar', ax=ax1, color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "ax1.set_title('Training Set Sentiment Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Sentiment')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Testing set distribution\n",
    "test_sentiment_dist.plot(kind='bar', ax=ax2, color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "ax2.set_title('Testing Set Sentiment Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Sentiment')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ea917",
   "metadata": {},
   "source": [
    "## 7. Train Sentiment Classification Models\n",
    "\n",
    "Now for the exciting part! We'll train multiple machine learning models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple machine learning models\n",
    "print(\"🤖 Training Multiple Machine Learning Models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define our models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(alpha=1.0),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Support Vector Machine': SVC(random_state=42, probability=True),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Train each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n🔄 Training {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    trained_models[model_name] = model\n",
    "    \n",
    "    print(f\"✅ {model_name} completed!\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    print(f\"   Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n🎉 All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3c866",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Performance Metrics\n",
    "\n",
    "Let's evaluate our models comprehensively and visualize their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d94e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison of all models\n",
    "print(\"📊 Model Comparison Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, results in model_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1_score'],\n",
    "        'Training Time (s)': results['training_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find the best model\n",
    "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_f1_score = comparison_df.loc[best_model_idx, 'F1-Score']\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name} (F1-Score: {best_f1_score:.4f})\")\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "comparison_df.plot(x='Model', y='Accuracy', kind='bar', ax=ax1, color='skyblue', legend=False)\n",
    "ax1.set_title('Model Accuracy Comparison', fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# F1-Score comparison\n",
    "comparison_df.plot(x='Model', y='F1-Score', kind='bar', ax=ax2, color='lightgreen', legend=False)\n",
    "ax2.set_title('Model F1-Score Comparison', fontweight='bold')\n",
    "ax2.set_ylabel('F1-Score')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Precision vs Recall\n",
    "ax3.scatter(comparison_df['Precision'], comparison_df['Recall'], s=100, alpha=0.7)\n",
    "for i, model in enumerate(comparison_df['Model']):\n",
    "    ax3.annotate(model, (comparison_df['Precision'].iloc[i], comparison_df['Recall'].iloc[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "ax3.set_xlabel('Precision')\n",
    "ax3.set_ylabel('Recall')\n",
    "ax3.set_title('Precision vs Recall', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "comparison_df.plot(x='Model', y='Training Time (s)', kind='bar', ax=ax4, color='coral', legend=False)\n",
    "ax4.set_title('Training Time Comparison', fontweight='bold')\n",
    "ax4.set_ylabel('Training Time (seconds)')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843972fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification report for the best model\n",
    "print(f\"📋 Detailed Classification Report for {best_model_name}:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_model = trained_models[best_model_name]\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y_test, best_predictions, output_dict=True)\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(f\"\\n📊 Confusion Matrix for {best_model_name}:\")\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sentiment_labels = sorted(df['sentiment'].unique())\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=sentiment_labels, yticklabels=sentiment_labels)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(f\"\\n📈 Per-Class Performance for {best_model_name}:\")\n",
    "for sentiment in sentiment_labels:\n",
    "    precision = class_report[sentiment]['precision']\n",
    "    recall = class_report[sentiment]['recall']\n",
    "    f1 = class_report[sentiment]['f1-score']\n",
    "    support = class_report[sentiment]['support']\n",
    "    \n",
    "    print(f\"\\n{sentiment.capitalize()}:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Support: {support}\")\n",
    "\n",
    "# Cross-validation for more robust evaluation\n",
    "print(f\"\\n🔄 Cross-Validation Results for {best_model_name}:\")\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "print(f\"CV F1-Scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983c530",
   "metadata": {},
   "source": [
    "## 9. Predict Sentiments on New Tweets\n",
    "\n",
    "Finally, let's create a prediction function and test our model on new, unseen tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vectorizer, preprocessing_func):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a given text using the trained model\n",
    "    \"\"\"\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocessing_func(text)\n",
    "    \n",
    "    # Transform using the fitted vectorizer\n",
    "    text_features = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(text_features)[0]\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(text_features)[0]\n",
    "        sentiment_labels = model.classes_\n",
    "        prob_dict = dict(zip(sentiment_labels, probabilities))\n",
    "    else:\n",
    "        prob_dict = None\n",
    "    \n",
    "    return prediction, prob_dict\n",
    "\n",
    "# Test on new tweets\n",
    "test_tweets = [\n",
    "    \"This movie is absolutely fantastic! I loved every minute of it! 🎬✨\",\n",
    "    \"Terrible service at this restaurant. Food was cold and staff was rude 😡\",\n",
    "    \"The weather today is okay. Not too hot, not too cold.\",\n",
    "    \"Just got my dream job! So excited to start this new chapter! 🎉\",\n",
    "    \"My flight got cancelled again. This airline is the worst! ✈️😤\",\n",
    "    \"Reading this book right now. It's decent, has some interesting parts.\",\n",
    "    \"Best concert ever! The band was incredible! 🎵🔥\",\n",
    "    \"This app keeps crashing. Very frustrating experience 📱\",\n",
    "    \"Had lunch with friends today. Nice time, good food.\",\n",
    "    \"Can't believe how amazing this vacation is! Paradise! 🏖️\"\n",
    "]\n",
    "\n",
    "print(\"🔮 Predicting Sentiments on New Tweets:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "# Store results for analysis\n",
    "prediction_results = []\n",
    "\n",
    "for i, tweet in enumerate(test_tweets, 1):\n",
    "    prediction, probabilities = predict_sentiment(tweet, best_model, tfidf_vectorizer, advanced_preprocess_text)\n",
    "    \n",
    "    print(f\"\\nTweet {i}:\")\n",
    "    print(f\"Text: {tweet}\")\n",
    "    print(f\"Predicted Sentiment: {prediction.upper()}\")\n",
    "    \n",
    "    if probabilities:\n",
    "        print(\"Confidence Scores:\")\n",
    "        for sentiment, prob in probabilities.items():\n",
    "            emoji = \"😞\" if sentiment == 'negative' else \"😐\" if sentiment == 'neutral' else \"😊\"\n",
    "            print(f\"  {emoji} {sentiment.capitalize()}: {prob:.3f}\")\n",
    "    \n",
    "    prediction_results.append({\n",
    "        'tweet': tweet,\n",
    "        'predicted_sentiment': prediction,\n",
    "        'probabilities': probabilities\n",
    "    })\n",
    "\n",
    "print(f\"\\n✅ Completed predictions using {best_model_name}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction confidence\n",
    "prediction_df = pd.DataFrame(prediction_results)\n",
    "\n",
    "# Extract confidence scores for visualization\n",
    "confidence_data = []\n",
    "for idx, row in prediction_df.iterrows():\n",
    "    if row['probabilities']:\n",
    "        for sentiment, prob in row['probabilities'].items():\n",
    "            confidence_data.append({\n",
    "                'tweet_id': idx + 1,\n",
    "                'sentiment': sentiment,\n",
    "                'probability': prob,\n",
    "                'is_predicted': sentiment == row['predicted_sentiment']\n",
    "            })\n",
    "\n",
    "confidence_df = pd.DataFrame(confidence_data)\n",
    "\n",
    "# Plot confidence scores\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create a pivot table for heatmap\n",
    "pivot_conf = confidence_df.pivot(index='tweet_id', columns='sentiment', values='probability')\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pivot_conf, annot=True, fmt='.3f', cmap='RdYlBu_r', \n",
    "            cbar_kws={'label': 'Confidence Score'})\n",
    "plt.title('Prediction Confidence Scores for Each Tweet', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Tweet ID')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of predictions\n",
    "prediction_summary = prediction_df['predicted_sentiment'].value_counts()\n",
    "print(f\"\\n📊 Prediction Summary:\")\n",
    "for sentiment, count in prediction_summary.items():\n",
    "    percentage = (count / len(prediction_df)) * 100\n",
    "    print(f\"{sentiment.capitalize()}: {count} tweets ({percentage:.1f}%)\")\n",
    "\n",
    "# Interactive prediction function\n",
    "print(f\"\\n🎮 Interactive Sentiment Analysis:\")\n",
    "print(\"You can now use the predict_sentiment function to analyze any text!\")\n",
    "print(\"Example usage:\")\n",
    "print(\"prediction, probabilities = predict_sentiment('Your text here', best_model, tfidf_vectorizer, advanced_preprocess_text)\")\n",
    "\n",
    "# Create a simple interface function\n",
    "def analyze_text(text):\n",
    "    \"\"\"Simple interface for sentiment analysis\"\"\"\n",
    "    prediction, probabilities = predict_sentiment(text, best_model, tfidf_vectorizer, advanced_preprocess_text)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {prediction.upper()}\")\n",
    "    if probabilities:\n",
    "        print(\"Confidence:\")\n",
    "        for sentiment, prob in probabilities.items():\n",
    "            print(f\"  {sentiment.capitalize()}: {prob:.3f}\")\n",
    "    \n",
    "    return prediction, probabilities\n",
    "\n",
    "print(f\"\\nYou can also use: analyze_text('Your tweet here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb071f65",
   "metadata": {},
   "source": [
    "## 🎉 Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a complete Twitter sentiment analysis system using NLP and machine learning. Let's summarize what we accomplished:\n",
    "\n",
    "### 🏆 What We Built\n",
    "1. **Data Processing Pipeline**: Loaded and cleaned Twitter data\n",
    "2. **Text Preprocessing**: Comprehensive cleaning and normalization of tweet text\n",
    "3. **Feature Extraction**: Converted text to numerical features using TF-IDF\n",
    "4. **Model Training**: Trained and compared multiple ML algorithms\n",
    "5. **Model Evaluation**: Comprehensive performance analysis with metrics and visualizations\n",
    "6. **Prediction System**: Created a production-ready sentiment classifier\n",
    "\n",
    "### 📊 Key Results\n",
    "- **Best Model**: {best_model_name}\n",
    "- **Accuracy**: {best_f1_score:.1%}\n",
    "- **Capabilities**: Classifies text into positive, negative, and neutral sentiments\n",
    "\n",
    "### 🚀 Next Steps\n",
    "- **Scale Up**: Train with larger, real-world Twitter datasets\n",
    "- **Deep Learning**: Experiment with neural networks (LSTM, BERT)\n",
    "- **Real-time Analysis**: Connect to Twitter API for live sentiment monitoring\n",
    "- **Multi-language**: Extend to support multiple languages\n",
    "- **Deployment**: Deploy as a web service or mobile app\n",
    "\n",
    "### 🛠 Tools Mastered\n",
    "- **Pandas**: Data manipulation and analysis\n",
    "- **Scikit-Learn**: Machine learning algorithms and evaluation\n",
    "- **NLTK**: Natural language processing\n",
    "- **Matplotlib/Seaborn**: Data visualization\n",
    "\n",
    "You now have a solid foundation in NLP and sentiment analysis! 🎓"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
